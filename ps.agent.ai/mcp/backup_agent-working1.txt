import asyncio
import os
from dotenv import load_dotenv
from langchain.agents import AgentExecutor
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, AnyMessage
from langgraph.graph import StateGraph, START, END, add_messages
from typing import TypedDict, Annotated
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.constants import START
from langgraph.managed import RemainingSteps
from loguru import logger
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent, ToolNode
from client.mcp_tools import get_mcp_tool

load_dotenv()

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_API_KEY"] = "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"
os.environ["LANGCHAIN_PROJECT"] = "customer-rag-demo"
os.environ[
    "OPENAI_API_KEY"] = "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"

# Define chat state
class ChatState(TypedDict):
    #messages: Annotated[list[BaseMessage], add_messages]
    messages: Annotated[list[AnyMessage], add_messages]
    remaining_steps: RemainingSteps = 25

# ---------------------------------------------------------------------------
# 2️⃣  Initialize LLM + MCP tool
# ---------------------------------------------------------------------------
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
mcp_tool = get_mcp_tool()
# llm_with_tools = llm.bind_tools([mcp_tool])
chatbot = create_react_agent(llm, tools=[mcp_tool],state_schema=ChatState)


#agent_executor = AgentExecutor(agent=chatbot,tools=[mcp_tool])
# ---------------------------------------------------------------------------
# 3️⃣  Build the agent using LangGraph’s prebuilt ReAct agent
# ---------------------------------------------------------------------------



# Define node logic
async def chat_node(state: ChatState):
    messages = state['messages']
    response = await chatbot.ainvoke({"messages": [{"role": "user", "content": messages[-1].content}]})

    print(f"chatbot response is {response}")
    return {"messages": response['messages']}



# Define graph
checkpointer = InMemorySaver()
graph = StateGraph(state_schema=ChatState,async_mode=True)
graph.add_node("chat_node", chat_node)
graph.add_edge(START, "chat_node")
graph.add_edge("chat_node", END)

chat_graph = graph.compile(checkpointer=checkpointer)
